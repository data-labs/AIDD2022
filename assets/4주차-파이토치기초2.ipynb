{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa5b91e",
   "metadata": {},
   "source": [
    "# PyTorch를 사용한 경사하강법 및 선형 회귀\n",
    "\n",
    "## 이 노트북에서 다룰 내용\n",
    "1. 선형 회귀 및 경사하강법 소개\n",
    "2. PyTorch 텐서를 사용하여 선형 회귀 모델 구현\n",
    "3. 경사하강법을 이용한 선형 회귀 모델 학습\n",
    "4. 내장된 PyTorch를 사용하여 경사하강법 및 선형 회귀 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329a6a0",
   "metadata": {},
   "source": [
    "시작하기 전에 필요한 라이브러리를 설치해야 한다.\n",
    "\n",
    "PyTorch의 설치는 운영 체제/클라우드 환경에 따라 다를 수 있다. \n",
    "자세한 설치 지침은 https://pytorch.org에서 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b51818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "\n",
    "# Linux / Binder\n",
    "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Windows\n",
    "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# MacOS\n",
    "# !pip install numpy torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9c8fe",
   "metadata": {},
   "source": [
    "## 선형 회귀 소개\n",
    "\n",
    "이 노트북에서는 기계 학습의 기본 알고리즘 중 하나인 *선형 회귀*에 대해 설명한다.\n",
    "\n",
    "지역의 평균 기온, 강우량, 습도(*입력 변수 또는 특징*)를 보고 사과와 오렌지(*목표 변수*)의 작물 수확량을 예측하는 모델을 만들 것이다. 훈련 데이터는 아래와 같다.\n",
    "\n",
    "\n",
    "![선형-회귀-훈련-데이터](https://i.imgur.com/6Ujttb4.png)\n",
    "\n",
    "선형 회귀 모델에서 각 목표 변수는 입력 변수의 가중합(weighted sum)으로 추정되며 편향(bias)이라고 하는 상수가 추가된다.\n",
    "\n",
    "```\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "```\n",
    "\n",
    "시각적으로 사과의 수확량은 온도, 강우량 및 습도의 선형 또는 평면 함수임을 의미한다.\n",
    "\n",
    "![선형 회귀 그래프](https://i.imgur.com/4DJ9f8X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c586b",
   "metadata": {},
   "source": [
    "선형 회귀의 *학습* 부분은 새로운 데이터에 대한 정확한 예측을 하기 위해 훈련 데이터를 사용하여 적절한 가중치 'w11, w12,... w23, b1 & b2'값의 집합을 찾아내는 것이다.\n",
    "\n",
    "_학습된_ 가중치는 해당 지역의 평균 온도, 강우량 및 습도를 사용하여 새 지역에서 사과와 오렌지의 수확량을 예측하는 데 사용된다.\n",
    "\n",
    "\n",
    "\n",
    "우리는 *경사하강법*이라는 최적화 기술을 사용하여 더 나은 예측을 위해 가중치를 약간 여러 번 조정하여 모델을 _훈련_시킬 것이다.\n",
    "\n",
    "Numpy와 PyTorch를 가져오는 것부터 시작하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d2dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a4cdf",
   "metadata": {},
   "source": [
    "## 훈련 데이터\n",
    "\n",
    "우리는 '입력'과 '목표'라는 두 개의 행렬을 사용하여 훈련 데이터를 나타낼 수 있으며, 각각 관찰당 하나의 행과 변수당 하나의 열이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc2a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21dc81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c2e9e",
   "metadata": {},
   "source": [
    "입력 변수와 대상 변수는 별도로 작업할 것이기 때문에 분리했다.\n",
    "\n",
    "또한 일반적으로 훈련 데이터로 작업하는 방식이기 때문에 numpy 배열을 만들었다.\n",
    "\n",
    "일부 CSV 파일을 numpy 배열로 읽고 일부 처리를 수행한 다음 PyTorch 텐서로 변환한다.\n",
    "\n",
    "배열을 PyTorch 텐서로 변환해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8769f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb2317",
   "metadata": {},
   "source": [
    "## 선형 회귀 모델\n",
    "\n",
    "가중치와 편향(`w11, w12,... w23, b1 & b2`)은 임의의 값으로 초기화되는 행렬로 나타낼 수도 있다.\n",
    "\n",
    "`w`의 첫 번째 행과 `b`의 첫 번째 요소는 첫 번째 대상 변수, 즉 사과의 수확량을 예측하는 데 사용되며 유사하게 두 번째는 오렌지 일때도 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3d91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0799,  1.5909, -1.4551],\n",
      "        [-0.5223,  0.9004,  1.1792]], requires_grad=True)\n",
      "tensor([-0.1269, -0.0251], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734429a4",
   "metadata": {},
   "source": [
    "`torch.randn`은 평균이 0이고 표준편차가 1인 [정규 분포](https://en.wikipedia.org/wiki/Normal_distribution)에서 무작위로 선택된 요소를 사용하여 주어진 모양으로 텐서를 생성한다.\n",
    "\n",
    "우리의 *모델*은 단순히 `입력`과 가중치 `w`(전치)의 행렬 곱셈을 수행하고 편향 `b`(각 관찰에 대해 복제됨)을 추가하는 함수다.\n",
    "\n",
    "![매트릭스 다중](https://i.imgur.com/WGXLFvA.png)\n",
    "\n",
    "다음과 같이 모델을 정의할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cfc0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc019f2d",
   "metadata": {},
   "source": [
    "`@`는 PyTorch에서 행렬 곱셈을 나타내고 `.t` 메서드는 텐서의 전치를 반환한다.\n",
    "\n",
    "입력 데이터를 모델에 전달하여 얻은 행렬은 대상 변수에 대한 예측 집합이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3304e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 38.0630,  72.8767],\n",
      "        [ 39.4773, 107.1464],\n",
      "        [121.7068, 143.5788],\n",
      "        [  6.2963,  29.0438],\n",
      "        [ 45.2311, 132.9165]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543809b",
   "metadata": {},
   "source": [
    "우리 모델의 예측을 실제 목표와 비교해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f0db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e595a",
   "metadata": {},
   "source": [
    "무작위 가중치와 편향으로 모델을 초기화했기 때문에 모델의 예측과 실제 목표 사이에 큰 차이가 있음을 알 수 있을 것이다.\n",
    "\n",
    "분명히, 무작위로 초기화된 모델이 *잘 작동*할 것이라고 기대할 수는 없을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633da3f",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "\n",
    "모델을 개선하기 전에 모델이 얼마나 잘 수행되고 있는지 평가할 방법이 필요하다. 다음 방법을 사용하여 모델의 예측을 실제 목표와 비교할 수 있다.\n",
    "\n",
    "* 두 행렬(`preds` 및 `targets`) 간의 차이를 계산한다.\n",
    "\n",
    "* 음수 값을 제거하기 위해 차분 행렬의 모든 요소를 제곱한다.\n",
    "\n",
    "* 결과 행렬의 요소 평균을 계산한다.\n",
    "\n",
    "결과는 **평균 제곱 오차**(MSE)로 알려진 단일 숫자이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7f373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c94eba",
   "metadata": {},
   "source": [
    "`torch.sum`은 텐서에 있는 모든 요소의 합을 반환한다.\n",
    "\n",
    "텐서의 `.numel` 메서드는 텐서의 요소 수를 반환한다.\n",
    "\n",
    "우리 모델의 현재 예측에 대한 평균 제곱 오차를 계산해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca517212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(605.1987, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33419f",
   "metadata": {},
   "source": [
    "결과를 해석하면, *평균적으로 예측의 각 요소는 손실의 제곱근만큼 실제 목표와 다르다*.\n",
    "\n",
    "그리고 우리가 예측하려는 숫자 자체가 50-200 범위에 있다는 점을 고려할 때 이는 매우 나쁘다.\n",
    "\n",
    "결과는 모델이 목표 변수를 예측하는 데 얼마나 나쁜지를 나타내기 때문에 *손실*이라고 말한다.\n",
    "\n",
    "이는 모델의 정보 손실을 나타낸다. 손실이 낮을수록 더 좋은 모델임을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327d2d9",
   "metadata": {},
   "source": [
    "## 그라디언트 계산\n",
    "\n",
    "PyTorch를 사용하면 손실 w.r.t의 기울기 또는 도함수를 자동으로 계산할 수 있다.\n",
    "\n",
    "`requires_grad`가 `True`로 설정되어 있기 때문에 가중치와 편향을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb3a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d6f1c",
   "metadata": {},
   "source": [
    "기울기는 각 텐서의 `.grad` 속성에 저장된다.\n",
    "\n",
    "손실 w.r.t의 미분에 유의하라. 가중치 행렬은 그 자체가 같은 차원의 행렬이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd6c9431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0799,  1.5909, -1.4551],\n",
      "        [-0.5223,  0.9004,  1.1792]], requires_grad=True)\n",
      "tensor([[-2088.0627, -2142.8301, -1579.3232],\n",
      "        [  367.6758,   629.0081,   362.0818]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3292f5b",
   "metadata": {},
   "source": [
    "## 손실을 줄이기 위해 가중치와 편향을 조정합니다.\n",
    "\n",
    "손실은 가중치와 편향의 [2차 함수](https://en.wikipedia.org/wiki/Quadratic_function)이며, 우리의 목표는 손실이 가장 낮은 가중치 집합을 찾는 것이다.\n",
    "\n",
    "개별 가중치 또는 편향 요소에 대한 손실 그래프를 플롯하면 아래 그림과 같이 표시된다.\n",
    "\n",
    "미적분학의 중요한 통찰력은 기울기가 손실의 변화율, 즉 손실 함수의 [slope](https://en.wikipedia.org/wiki/Slope) w.r.t를 나타낸다.\n",
    "\n",
    "그래디언트 요소가 **양수**인 경우:\n",
    "\n",
    "* **가중치 요소의 값을 약간 증가**하면 손실이 **증가**\n",
    "* **가중치 요소의 값을 약간 감소**하면 손실이 **감소**\n",
    "\n",
    "![사위 그라데이션](https://i.imgur.com/WLzJ4xP.png)\n",
    "\n",
    "그래디언트 요소가 **음수**인 경우:\n",
    "\n",
    "* **가중치 요소의 값을 약간 증가**하면 손실이 **감소**\n",
    "* **가중치 요소의 값을 약간 낮추면** 손실이 **증가**\n",
    "\n",
    "![음수=그라디언트](https://i.imgur.com/dvG2fxU.png)\n",
    "\n",
    "가중치 요소를 변경하여 손실의 증가 또는 감소는 손실 w.r.t의 기울기에 비례한다. \n",
    "이러한 내용은 _gradient_를 따라 _descending_ 모델을 개선하는 데 사용할 _gradient descent_ 최적화 알고리즘의 기초이다.\n",
    "\n",
    "손실 w.r.t의 도함수에 비례하는 소량을 각 가중치 요소에서 뺄 수 있다. 이 요소로 손실을 약간 줄이는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0148d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2088.0627, -2142.8301, -1579.3232],\n",
       "        [  367.6758,   629.0081,   362.0818]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920ef072",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d613563",
   "metadata": {},
   "source": [
    "가중치를 너무 많이 수정하지 않도록 매우 작은 수(이 경우 '10^-5')로 그래디언트에 곱한다.\n",
    "\n",
    "이로써, 우리는 내리막 길로 작은 발걸음을 내디뎠다고 볼 수 있다.\n",
    "\n",
    "이 수치를 알고리즘의 *학습률*이라고 한다.\n",
    "\n",
    "우리는 가중치와 편향을 업데이트하는 동안 그래디언트를 추적, 계산 또는 수정해서는 안 된다고 PyTorch에 나타내기 위해 `torch.no_grad`를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72226378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(605.1987, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the loss is actually lower\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52892a",
   "metadata": {},
   "source": [
    "계속 진행하기 전에 `.zero_()` 메서드를 호출하여 그래디언트를 0으로 재설정해야한다.\n",
    "\n",
    "PyTorch가 그라디언트를 축적하기 때문에 이 작업을 수행해야한다.\n",
    "\n",
    "그렇지 않으면 다음에 손실에 대해 `.backward`를 호출할 때 새 그라디언트 값이 기존 그라디언트에 추가되어 예기치 않은 결과가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1491d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b465b4d",
   "metadata": {},
   "source": [
    "## 경사하강법을 사용한 모델 훈련\n",
    "\n",
    "이제 우리는 경사하강 최적화 알고리즘을 사용하여 손실을 줄이고 모델을 개선할 수 있다. \n",
    "따라서 다음 단계를 사용하여 모델을 _학습(train)_할 수 있을 것이다.\n",
    "\n",
    "1. 예측 생성\n",
    "2. 손실 계산\n",
    "3. 가중치와 편향으로 기울기 계산\n",
    "4. 기울기에 비례하는 소량을 빼서 가중치 조정\n",
    "5. 그라디언트를 0으로 재설정\n",
    "\n",
    "위의 단계를 단계별로 구현해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d36fe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 41.7023,  72.0311],\n",
      "        [ 44.2742, 106.0265],\n",
      "        [127.3110, 142.2060],\n",
      "        [  9.9322,  28.2643],\n",
      "        [ 49.8347, 131.8054]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48862f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(494.8894, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85a296dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1712.4089, -1739.7845, -1330.2585],\n",
      "        [  279.8259,   533.1392,   303.2268]])\n",
      "tensor([-21.5891,   4.0666])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d2887",
   "metadata": {},
   "source": [
    "위에서 계산된 그래디언트를 사용하여 가중치와 편향을 업데이트해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35b9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4554c9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0419,  1.6297, -1.4260],\n",
      "        [-0.5288,  0.8888,  1.1726]], requires_grad=True)\n",
      "tensor([-0.1264, -0.0252], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6225a2",
   "metadata": {},
   "source": [
    "새로운 가중치와 편향을 사용하면 모델의 손실이 낮을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6d7b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(420.3148, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b8d27",
   "metadata": {},
   "source": [
    "기울기 하강법을 사용하여 가중치와 편향을 약간 조정하는 것만으로 손실을 줄일 수 있다는 것을 확인할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea190a15",
   "metadata": {},
   "source": [
    "## 반복을 통한 훈련\n",
    "\n",
    "손실을 더 줄이기 위해 기울기를 사용하여 가중치와 편향을 조정하는 과정을 여러 번 반복할 수 있다.\n",
    "\n",
    "각 반복을 _epoch_라고 한다.  이제 100 Epoch 동안 모델을 훈련시켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80e2d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8706685",
   "metadata": {},
   "source": [
    "다시 한 번 손실이 더 낮아졌는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cae838a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(170.9299, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44e1a1",
   "metadata": {},
   "source": [
    "손실은 이제 초기 값보다 훨씬 낮다. 모델의 예측을 보고 타겟(실제 값)과 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9d11dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.5064,  69.5755],\n",
       "        [ 69.1907, 101.6759],\n",
       "        [146.1393, 131.8502],\n",
       "        [ 27.5950,  32.9276],\n",
       "        [ 75.4510, 123.2680]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c81dfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53994fc",
   "metadata": {},
   "source": [
    "이제 예측이 목표 변수(타겟)에 매우 가깝습니다.\n",
    "\n",
    "몇 epoch를 더 훈련하면 더 나은 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fea0f",
   "metadata": {},
   "source": [
    "## PyTorch 내장을 사용한 선형 회귀\n",
    "\n",
    "우리는 앞에서 몇 가지 기본 텐서 연산을 사용하여 선형 회귀 및 경사 하강 모델을 구현했다.\n",
    "\n",
    "그러나 이것은 딥 러닝의 일반적인 패턴이기 때문에 PyTorch는 몇 줄의 코드로 모델을 쉽게 만들고 훈련할 수 있도록 여러 내장 함수와 클래스를 제공한다.\n",
    "\n",
    "신경망 구축을 위한 유틸리티 클래스가 포함된 PyTorch에서 `torch.nn` 패키지를 가져오는 것으로 시작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3d5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35cae6",
   "metadata": {},
   "source": [
    "이전과 마찬가지로 입력, 목표 및 행렬을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d27ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d084c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3649644",
   "metadata": {},
   "source": [
    "## 데이터세트와 데이터로더\n",
    "\n",
    "우리는 `입력(inputs)`과 `출력, 목표변수(targets)`의 행에 튜플로 액세스할 수 있는 `TensorDataset`을 생성할 수 있다.\n",
    "PyTorch에서 다양한 유형의 데이터 세트 작업을 위한 표준 API를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2575ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2aaee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88509da6",
   "metadata": {},
   "source": [
    "`TensorDataset`을 사용하면 배열 인덱싱 표기법(위 코드의 '[0:3]')을 사용하여 훈련 데이터의 작은 섹션에 액세스할 수 있다.\n",
    "\n",
    "두 개의 요소가 있는 튜플을 반환한다.\n",
    "\n",
    "첫 번째 요소에는 선택한 행에 대한 입력 변수가 포함되고 두 번째 요소에는 대상이 포함된다.\n",
    "\n",
    "또한 훈련하는 동안 데이터를 미리 정의된 크기의 배치로 분할할 수 있는 `DataLoader`를 만들 것이다. \n",
    "\n",
    "`DataLoader`는 데이터 셔플링 및 무작위 샘플링과 같은 기타 유틸리티도 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1eca4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7e9c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598ca8e",
   "metadata": {},
   "source": [
    "우리는 `for` 루프에서 데이터 로더를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34bdb300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.,  44.,  37.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [ 68.,  97.,  70.],\n",
      "        [ 92.,  87.,  64.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[ 21.,  38.],\n",
      "        [118., 132.],\n",
      "        [102., 120.],\n",
      "        [ 82., 100.],\n",
      "        [ 22.,  37.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f259230",
   "metadata": {},
   "source": [
    "각 반복에서 데이터 로더는 지정된 배치 크기의 데이터 배치 하나를 반환한다.\n",
    "\n",
    "`shuffle`이 `True`로 설정되어 있으면 배치를 생성하기 전에 훈련 데이터를 섞을 수 있다.\n",
    "\n",
    "셔플링은 최적화 알고리즘에 대한 입력을 무작위화하여 손실을 더 빠르게 줄이는 데 도움이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c4ed4",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "\n",
    "가중치와 편향을 수동으로 초기화하는 대신 PyTorch의 `nn.Linear` 클래스를 사용하여 모델을 정의할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a6de165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.3217, 0.2614, 0.1505],\n",
      "        [0.1617, 0.1871, 0.3047]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5377,  0.0781], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24793852",
   "metadata": {},
   "source": [
    "PyTorch 모델에는 모델에 있는 모든 가중치와 편향 행렬을 포함하는 리스트를 반환하는 유용한 `.parameters` 메서드가 있다.\n",
    "\n",
    "선형 회귀 모델의 경우 하나의 가중치 행렬과 하나의 편향 행렬을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e87af195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.3217, 0.2614, 0.1505],\n",
       "         [0.1617, 0.1871, 0.3047]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5377,  0.0781], requires_grad=True)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b13d5",
   "metadata": {},
   "source": [
    "모델을 사용하여 이전과 같은 방식으로 예측을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac17f7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46.9318, 37.5157],\n",
       "        [61.3721, 50.7525],\n",
       "        [71.2055, 56.8847],\n",
       "        [49.0864, 35.8856],\n",
       "        [57.2877, 50.5206],\n",
       "        [46.9922, 37.4902],\n",
       "        [61.2612, 50.8700],\n",
       "        [71.6777, 57.3510],\n",
       "        [49.0260, 35.9110],\n",
       "        [57.1164, 50.6636],\n",
       "        [46.8209, 37.6332],\n",
       "        [61.4324, 50.7271],\n",
       "        [71.3164, 56.7672],\n",
       "        [49.2576, 35.7426],\n",
       "        [57.2273, 50.5460]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacbf99a",
   "metadata": {},
   "source": [
    "## 손실 함수\n",
    "\n",
    "손실 함수를 수동으로 정의하는 대신 내장 손실 함수 `mse_loss`를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5df80750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57520c",
   "metadata": {},
   "source": [
    "`nn.functional` 패키지에는 많은 유용한 손실 기능과 기타 여러 유틸리티가 포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "055b4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5f27c",
   "metadata": {},
   "source": [
    "우리 모델의 현재 예측에 대한 손실을 계산해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1c177b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1957.0083, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6069ad",
   "metadata": {},
   "source": [
    "## 최적화\n",
    "\n",
    "그라디언트를 사용하여 모델의 가중치 및 편향을 수동으로 조작하는 대신 최적화 `optim.SGD`를 사용할 수 있다.\n",
    "\n",
    "SGD는 `\"stochastic gradient descent\"`의 약자이다.\n",
    "\n",
    "_stochastic_이라는 용어는 샘플이 단일 그룹 대신 무작위 배치로 선택됨을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6638611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82439106",
   "metadata": {},
   "source": [
    "`model.parameters()`는 `optim.SGD`에 인수로 전달되어 옵티마이저가 업데이트 단계에서 어떤 행렬을 수정해야 하는지 알 수 있다.\n",
    "\n",
    "또한 매개변수가 수정되는 양을 제어하는 학습률을 지정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ecb74",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "이제 모델을 훈련할 준비가 되었다. 우리는 경사 하강법을 구현하기 위해 동일한 프로세스를 따를 것이다.\n",
    "\n",
    "1. 예측 생성\n",
    "\n",
    "2. 손실 계산\n",
    "\n",
    "3. 가중치와 편향으로 기울기 계산\n",
    "\n",
    "4. 기울기에 비례하는 소량을 빼서 가중치를 조정\n",
    "\n",
    "5. 그라디언트를 0으로 재설정\n",
    "\n",
    "유일한 변경 사항은 모든 반복에서 전체 교육 데이터를 처리하는 대신 배치단위의 데이터를 처리한다는 것이다.\n",
    "\n",
    "주어진 epoch 수에 대해 모델을 훈련시키는 유틸리티 함수 `fit`을 정의해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "344e2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221f440",
   "metadata": {},
   "source": [
    "위에서 주의해야 할 사항:\n",
    "\n",
    "* 우리는 이전에 정의한 데이터 로더를 사용하여 모든 반복에 대한 데이터 배치를 가져온다.\n",
    "\n",
    "* 매개변수(가중치 및 편향)를 수동으로 업데이트하는 대신 `opt.step`을 사용하여 업데이트를 수행하고 `opt.zero_grad`를 사용하여 그래디언트를 0으로 재설정한다.\n",
    "\n",
    "* 훈련 진행 상황을 추적하기 위해 매 10번째 epoch에 대한 마지막 데이터 배치의 손실을 인쇄하는 로그 문도 추가했다. `loss.item`은 손실 텐서에 저장된 실제 값을 반환한다.\n",
    "\n",
    "100 Epoch 동안 모델을 훈련시키자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42c2b87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 331.0684\n",
      "Epoch [20/100], Loss: 423.3690\n",
      "Epoch [30/100], Loss: 294.8078\n",
      "Epoch [40/100], Loss: 98.8626\n",
      "Epoch [50/100], Loss: 88.5521\n",
      "Epoch [60/100], Loss: 43.0958\n",
      "Epoch [70/100], Loss: 50.0133\n",
      "Epoch [80/100], Loss: 15.1282\n",
      "Epoch [90/100], Loss: 35.0053\n",
      "Epoch [100/100], Loss: 19.7186\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e131f",
   "metadata": {},
   "source": [
    "모델을 사용하여 예측을 생성하고 목표에 가까운지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e5f99b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.1850,  71.5359],\n",
       "        [ 80.6428,  99.1659],\n",
       "        [119.2262, 134.1587],\n",
       "        [ 28.4053,  44.0335],\n",
       "        [ 94.9190, 112.4586],\n",
       "        [ 57.0892,  70.5703],\n",
       "        [ 80.1648,  98.9442],\n",
       "        [119.3931, 134.6269],\n",
       "        [ 29.5011,  44.9991],\n",
       "        [ 95.5367, 113.2026],\n",
       "        [ 57.7069,  71.3143],\n",
       "        [ 79.5471,  98.2002],\n",
       "        [119.7043, 134.3803],\n",
       "        [ 27.7876,  43.2895],\n",
       "        [ 96.0148, 113.4242]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84b6f1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ba382",
   "metadata": {},
   "source": [
    "실제로, 예측은 우리의 목표와 근접한 값을 갖는다.\n",
    "\n",
    "이제, 우리는 한 지역의 평균 기온, 강우량, 습도를 보고 사과와 오렌지의 수확량을 예측할 수 있도록 합리적으로 좋은 모델을 훈련했다고 말할 수 있을 것이다.\n",
    "\n",
    "단일 입력 행을 포함하는 배치를 전달하여 새 지역의 작물 수확량을 예측하는 데 사용할 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72e786cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[54.6451, 68.5552]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e8dc3",
   "metadata": {},
   "source": [
    "사과의 예상 수확량은 헥타르당 54.3톤이고 오렌지는 헥타르당 68.3톤이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f16e4",
   "metadata": {},
   "source": [
    "## 기계 학습 vs. 명시적 프로그래밍\n",
    "\n",
    "이 노트북에서 우리가 취한 접근 방식은 여러분이 알고 있는 프로그래밍과 매우 다르다.일반적으로 우리는 일부 입력을 받고 일부 작업을 수행하고 결과를 반환하는 프로그램을 작성한다.\n",
    "\n",
    "그러나 이 노트북에서는 일부 알려지지 않은 매개변수(가중치 및 편향)를 사용하여 표현되는 입력과 출력 간의 특정 관계를 가정하는 `\"모델\"`을 정의했다. 그런 다음 모델에 일부 알려진 입력 및 출력을 보여주고 모델을 _train_하여 알려지지 않은 매개변수에 대한 좋은 값을 제시했다. 훈련을 마치면 모델을 사용하여 새 입력에 대한 출력을 계산할 수 있다.\n",
    "\n",
    "\n",
    "이러한 프로그래밍 패러다임은 _기계 학습(Machine Learning)_으로 알려져 있으며, 여기서 데이터를 사용하여 입력과 출력 간의 관계를 파악하는 방식으로 동작한다. _딥 러닝(Deep Learning)_은 행렬 연산, 비선형 활성화 함수 및 경사하강법을 사용하여 모델을 구축하고 훈련하는 기계 학습의 한 분야이다.\n",
    "\n",
    "Tesla Motors의 AI 이사인 Andrej Karpathy는 [Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35)이라는 제목으로 이 주제에 대한 훌륭한 블로그 게시물이 있다.\n",
    "\n",
    "Francois Chollet의 책 [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)의 이 그림은 명시적 프로그래밍과 기계 학습의 차이점에 초점을 두었다.\n",
    "\n",
    "![](https://i.imgur.com/oJEQe7k.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c528a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
