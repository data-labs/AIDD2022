{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46203b99",
   "metadata": {},
   "source": [
    "# PyTorch에서 이미지를 사용한 로지스틱 회귀\n",
    "\n",
    "## 이 노트북에서 다룰 내용\n",
    "1. PyTorch에서 이미지 작업(MNIST 데이터 세트 사용)\n",
    "2. 데이터 세트를 훈련, 검증 및 테스트 세트로 분할\n",
    "3. `nn.Module` 클래스를 확장하여 사용자 정의 로직으로 PyTorch 모델 생성\n",
    "4. Softmax를 사용하여 모델 출력을 확률로 해석하고 예측된 레이블 선택\n",
    "5. 분류 문제에 대한 유용한 평가 지표(정확도) 및 손실 함수(교차 엔트로피) 선택\n",
    "6. 검증 세트를 사용하여 모델을 평가하는 훈련 루프 설정\n",
    "7. 무작위로 선택한 예제에서 수동으로 모델 테스트\n",
    "8. 처음부터 다시 학습하지 않도록 모델 체크포인트 저장 및 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02656a05",
   "metadata": {},
   "source": [
    "## 이미지 작업\n",
    "\n",
    "이 노트북에서는 PyTorch 및 선형 회귀에 대한 기존 지식을 사용하여 매우 다른 종류의 문제인 *이미지 분류*를 해결할 것이다.\n",
    "\n",
    "우리는 유명한 [*MNIST 손으로 쓴 숫자 데이터베이스*](http://yann.lecun.com/exdb/mnist/)를 훈련 데이터 세트로 사용할 것이다.\n",
    "\n",
    "손으로 쓴 숫자(0~9)의 28픽셀 x 28픽셀 그레이스케일 이미지와 각 이미지가 나타내는 숫자를 나타내는 레이블로 구성된다.\n",
    "\n",
    "다음은 데이터세트의 샘플 이미지다.\n",
    "\n",
    "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316e1a3",
   "metadata": {},
   "source": [
    "먼저 `torch`와 `torchvision`을 설치하고 임포트 한다.\n",
    "\n",
    "`torchvision`에는 이미지 데이터 작업을 위한 몇 가지 유틸리티가 포함되어 있다.\n",
    "\n",
    "또한 MNIST와 같은 인기 있는 데이터 세트를 자동으로 다운로드하고 가져올 수 있는 도우미 클래스를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869f141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the appropriate command for your operating system, if required\n",
    "\n",
    "# Linux / Binder\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# Windows\n",
    "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# MacOS\n",
    "# !pip install numpy matplotlib torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117fd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8665b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a972a8e51fc54759b43c77dc7834c9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091ae97421e945f2aa223e859101820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7e7567f8ed4c8ab25b942f82b818bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa94796cf36480599f740c6bcaac1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dilab/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630788697/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde07c6",
   "metadata": {},
   "source": [
    "이 명령문이 처음 실행되면 노트북 옆의 `data/` 디렉토리에 데이터를 다운로드하고 PyTorch `Dataset`을 생성한다.\n",
    "\n",
    "데이터 세트의 크기를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d404a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9702b52",
   "metadata": {},
   "source": [
    "데이터 세트에는 모델을 훈련하는 데 사용할 60,000개의 이미지가 있다.\n",
    "\n",
    "또한 모델을 평가하는데 사용되는 10,000개 이미지의 추가 테스트 세트가 있다.\n",
    "\n",
    "`train=False`를 생성자에 전달하여 `MNIST` 클래스를 사용하여 테스트 데이터 세트를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a105d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd49f8",
   "metadata": {},
   "source": [
    "훈련 데이터 세트의 샘플 요소를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8a1cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F7B80BA4AC0>, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11687369",
   "metadata": {},
   "source": [
    "28x28px(픽셀)의 이미지와 레이블로 구성된 쌍이다.\n",
    "\n",
    "이미지는 파이썬 이미징 라이브러리 [Pillow](https://pillow.readthedocs.io/en/stable/)의 일부인 `PIL.Image.Image` 클래스의 객체다.\n",
    "\n",
    "Python의 데이터 과학을 위한 사실상의 플로팅 및 그래프 라이브러리인 [`matplotlib`](https://matplotlib.org/)를 사용하여 Jupyter 내에서 이미지를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dab95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da40cb9",
   "metadata": {},
   "source": [
    "`%matplotlib inline` 문은 Jupyter에 노트북 내에서 그래프를 플롯하기를 원한다는 것을 나타낸다.\n",
    "\n",
    "이 줄이 없으면 Jupyter는 이미지를 팝업으로 표시한다.\n",
    "\n",
    "`%`로 시작하는 명령문을 매직 명령이라고 하며 Jupyter 자체의 동작을 구성하는 데 사용다. 여기에서 매직 명령의 전체 리스트를 찾을 수 있다.\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/magics.html .\n",
    "\n",
    "데이터 세트에서 몇 가지 이미지를 살펴보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca299daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uty0Adev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpHPQKowSG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7rsE0CXJhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7EmHAGrRNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTSUi1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7i7VgF0o+1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbt6t55/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "453da845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71420ee5",
   "metadata": {},
   "source": [
    "이러한 이미지는 크기가 상대적으로 작으며 숫자를 인식하는 것은 사람의 눈으로도 때때로 어려울 수 있다.\n",
    "\n",
    "이러한 이미지를 보는 것이 유용하지만 여기에는 한 가지 문제가 있다. PyTorch는 이미지를 처리하는 라이브러리가 따로 존재하지 않는다.\n",
    "\n",
    "그렇기에 우리는 이미지를 텐서로 변환하고, 데이터 세트를 생성하는 동안 특정한 변환 작업을 지정해 이미지 데이터를 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5abe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4cd518",
   "metadata": {},
   "source": [
    "PyTorch 데이터 세트를 사용하면 이미지가 로드될 때 이미지에 적용되는 하나 이상의 변환 기능을 지정할 수 있다.\n",
    "\n",
    "`torchvision.transforms` 모듈에는 이러한 사전정의 기능이 많이 포함되어 있다.\n",
    "\n",
    "`ToTensor` 변환을 사용하여 이미지를 PyTorch 텐서로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b91661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', \n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0500172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c94e69",
   "metadata": {},
   "source": [
    "이제 이미지가 1x28x28 텐서로 변환된다. 첫 번째 차원은 색상 채널을 추적한다.\n",
    "\n",
    "두 번째 및 세 번째 차원은 각각 이미지의 높이와 너비에 따른 픽셀을 나타낸다.\n",
    "\n",
    "MNIST 데이터 세트의 이미지는 회색조이므로 채널이 하나만 있다.\n",
    "\n",
    "다른 데이터 세트에는 색상이 있는 이미지가 있으며 이 경우 빨간색, 녹색 및 파란색(RGB)의 세 가지 채널이 있다.\n",
    "\n",
    "텐서 내부의 몇 가지 샘플 값을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b92732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0,10:15,10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95dcf2",
   "metadata": {},
   "source": [
    "값의 범위는 0에서 1까지이며, `0`은 검정, `1`은 흰색, 서로 다른 회색 음영 사이의 값이다.\n",
    "\n",
    "`plt.imshow`를 사용하여 텐서를 이미지로 그릴 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8534b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJRElEQVR4nO3dz2ucBR7H8c9n04qiCx7qQZrSiohsEVahFKEHoQjWKnpVqF7UXFaoIIge/QfEi5egYsFSEfQg6iIFFRGsGjUWu1GoPxaLQncprXpRaj97mGHpuknzzHSeeeb58n5BIJMZMh9K3n1mJuEZJxGAOv7U9QAAk0XUQDFEDRRD1EAxRA0Us6GNb2q7Ny+pb926tesJI9m0aVPXE0by7bffdj2hsVOnTnU9YSRJvNrX3cavtGzHXvX+Zs7i4mLXE0by4IMPdj1hJPv27et6QmMHDx7sesJI1oqah99AMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxjaK2vcf2V7aP23687VEAxrdu1LbnJD0j6XZJ2yXda3t728MAjKfJkXqnpONJvknym6SXJN3d7iwA42oS9WZJ3593+cTwa//D9oLtJdtLkxoHYHRNThG82hkL/+8UpEkWJS1K/TpFMFBNkyP1CUlbzrs8L+mHduYAuFhNov5Y0nW2r7F9iaR7JL3W7iwA41r34XeSs7YflvSWpDlJzyc51voyAGNp9LY7Sd6U9GbLWwBMAH9RBhRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMY1OkjCOpB/nHjxz5kzXE0p76KGHup7Q2KFDh7qe0Ni5c+fWvI4jNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UMy6Udt+3vZJ219MYxCAi9PkSP2CpD0t7wAwIetGneQ9SaemsAXABPCcGihmYmcTtb0gaWFS3w/AeCYWdZJFSYuSZLsf5wcGCuLhN1BMk19pHZL0gaTrbZ+w/UD7swCMa92H30nuncYQAJPBw2+gGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBopxMvnTifXpHGWXX3551xNG8sYbb3Q9YSS33HJL1xMau+2227qe0NiRI0d05swZr3YdR2qgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKWTdq21tsv2N7xfYx2/unMQzAeDY0uM1ZSY8m+dT2nyV9Yvtwkn+0vA3AGNY9Uif5Mcmnw89/lrQiaXPbwwCMp8mR+r9sb5N0k6QPV7luQdLCZGYBGFfjqG1fIekVSY8k+emP1ydZlLQ4vG1vThEMVNPo1W/bGzUI+mCSV9udBOBiNHn125Kek7SS5Kn2JwG4GE2O1Lsk3Sdpt+3l4cfelncBGNO6z6mTvC9p1bf3ADB7+IsyoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKcTL5cwRy4sH2XHvttV1PGMny8nLXExo7ffp01xMa27t3r44ePbrqyUs4UgPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8WsG7XtS21/ZPtz28dsPzmNYQDGs6HBbX6VtDvJL7Y3Snrf9t+THGl5G4AxrBt1Bicx+2V4cePwg3OQATOq0XNq23O2lyWdlHQ4yYetrgIwtkZRJ/k9yY2S5iXttH3DH29je8H2ku2lCW8EMIKRXv1OclrSu5L2rHLdYpIdSXZMZhqAcTR59fsq21cOP79M0q2Svmx5F4AxNXn1+2pJB2zPafCfwMtJXm93FoBxNXn1+6ikm6awBcAE8BdlQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0+TMJ5ghX3/9ddcTRnL//fd3PaGxAwcOdD2hsQ0b1k6XIzVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFNI7a9pztz2y/3uYgABdnlCP1fkkrbQ0BMBmNorY9L+kOSc+2OwfAxWp6pH5a0mOSzq11A9sLtpdsL01iGIDxrBu17TslnUzyyYVul2QxyY4kOya2DsDImhypd0m6y/Z3kl6StNv2i62uAjC2daNO8kSS+STbJN0j6e0k+1pfBmAs/J4aKGakt91J8q6kd1tZAmAiOFIDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVCMk0z+m9r/kvTPCX/bTZL+PeHv2aY+7e3TVqlfe9vaujXJVatd0UrUbbC91KczlfZpb5+2Sv3a28VWHn4DxRA1UEyfol7sesCI+rS3T1ulfu2d+tbePKcG0EyfjtQAGiBqoJheRG17j+2vbB+3/XjXey7E9vO2T9r+oust67G9xfY7tldsH7O9v+tNa7F9qe2PbH8+3Ppk15uasD1n+zPbr0/rPmc+attzkp6RdLuk7ZLutb2921UX9IKkPV2PaOispEeT/EXSzZL+NsP/tr9K2p3kr5JulLTH9s3dTmpkv6SVad7hzEctaaek40m+SfKbBu+8eXfHm9aU5D1Jp7re0USSH5N8Ovz8Zw1++DZ3u2p1GfhleHHj8GOmX+W1PS/pDknPTvN++xD1Zknfn3f5hGb0B6/PbG+TdJOkDzuesqbhQ9llSSclHU4ys1uHnpb0mKRz07zTPkTtVb420/9D943tKyS9IumRJD91vWctSX5PcqOkeUk7bd/Q8aQ12b5T0skkn0z7vvsQ9QlJW867PC/ph462lGN7owZBH0zyatd7mkhyWoN3X53l1y52SbrL9ncaPGXcbfvFadxxH6L+WNJ1tq+xfYkGb3z/WsebSrBtSc9JWknyVNd7LsT2VbavHH5+maRbJX3Z6agLSPJEkvkk2zT4mX07yb5p3PfMR53krKSHJb2lwQs5Lyc51u2qtdk+JOkDSdfbPmH7ga43XcAuSfdpcBRZHn7s7XrUGq6W9I7toxr8R384ydR+TdQn/JkoUMzMH6kBjIaogWKIGiiGqIFiiBoohqiBYogaKOY/GaruA892b2gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the image by passing in the 28x28 matrix\n",
    "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb098d",
   "metadata": {},
   "source": [
    "채널 차원 없이 28x28 행렬만 `plt.imshow`에 전달해야 한다.\n",
    "\n",
    "또한 그레이스케일(흑백) 이미지를 보고 싶다는 것을 나타내기 위해 컬러 맵(`cmap=gray`)으로 세팅해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5effa1f",
   "metadata": {},
   "source": [
    "## 교육(Training) 및 검증(Validation) 데이터 세트\n",
    "\n",
    "기계 학습 모델을 구축하는 동안 데이터 세트를 세 부분으로 나누는 것이 일반적이다.\n",
    "\n",
    "1. **훈련 세트(Training set)** - 모델 훈련에 사용 즉, 손실을 계산하고 경사하강법을 사용하여 모델의 가중치를 조정\n",
    "\n",
    "2. **검증 세트(Validation set)** - 훈련 중 모델을 평가하고, 하이퍼파라미터(학습률 등)를 조정하고, 모델의 최상의 버전을 선택하는 데 사용\n",
    "\n",
    "3. **테스트 세트(Test set)** - 다양한 모델 또는 접근 방식을 비교하고 모델의 최종 정확도를 보고하는 데 사용\n",
    "\n",
    "MNIST 데이터셋에는 60,000개의 훈련 이미지와 10,000개의 테스트 이미지가 있다. 테스트 세트는 표준화되어 여러 연구자가 동일한 이미지 컬렉션에 대해 모델 결과를 보고할 수 있다.\n",
    "\n",
    "\n",
    "사전 정의된 검증 세트가 없기 때문에 60,000개의 이미지를 훈련 및 검증 데이터 세트로 수동으로 분할해야 한다. 유효성 검사를 위해 무작위로 선택한 10,000개의 이미지를 따로 보관해 보자. PyTorch의 `random_spilt` 메서드를 사용하여 이 작업을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cc6110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278850b4",
   "metadata": {},
   "source": [
    "검증 세트를 생성하기 위해 무작위 샘플을 선택하는 것은 필수다. 훈련 데이터는 종종 대상 레이블, 즉 0, 1, 2 등의 이미지로 정렬된다. 이미지의 마지막 20%를 사용하여 유효성 검사 세트를 생성하면 8과 9로만 구성된다. 반면, 훈련 세트에는 8 또는 9가 포함되지 않습니다.\n",
    "\n",
    "이러한 훈련 검증은 유용한 모델을 훈련하는 것을 불가능하게 만든다. 이제 데이터를 일괄적으로 로드하는 데 도움이 되는 데이터 로더를 만들 수 있다. 배치 크기 128을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758a7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acee7a",
   "metadata": {},
   "source": [
    "각 epoch에서 생성된 배치가 서로 다른지 확인하기 위해 훈련 데이터 로더에 `shuffle=True`를 설정했다.\n",
    "\n",
    "이 무작위화는 훈련 과정을 일반화하고 속도를 높이는 데 도움이 된다.\n",
    "\n",
    "반면 검증 데이터 로더는 모델 평가에만 사용되기 때문에 이미지를 섞을 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3863e01",
   "metadata": {},
   "source": [
    "## 모델\n",
    "\n",
    "이제 데이터 로더를 준비했으므로 모델을 정의할 수 있다.\n",
    "\n",
    "* **로지스틱 회귀** 모델은 선형 회귀 모델과 거의 동일하다. 여기에는 가중치와 편향 행렬이 포함되어 있으며 간단한 행렬 연산(`pred = x @ w.t() + b`)을 사용하여 출력을 얻는다.\n",
    "\n",
    "* 선형 회귀에서 했던 것처럼 행렬을 수동으로 생성하고 초기화하는 대신 `nn.Linear`를 사용하여 모델을 생성할 수 있다.\n",
    "\n",
    "* `nn.Linear`는 각 학습 예제가 벡터가 될 것으로 예상하므로 각 `1x28x28` 이미지 텐서는 모델에 전달되기 전에 크기 784 `(28*28)`의 벡터로 _평탄화(펼쳐)_된다.\n",
    "\n",
    "* 각 이미지의 출력은 크기가 10인 벡터이며 각 요소는 특정 대상 레이블(즉, 0에서 9)의 확률을 나타낸다. 이미지에 대해 예측된 레이블은 단순히 확률이 가장 높은 레이블이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c05fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d734af6",
   "metadata": {},
   "source": [
    "물론 이 모델은 매개변수의 수 면에서 이전 모델보다 훨씬 크다.\n",
    "\n",
    "가중치와 편향에 대해 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c5687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0266, -0.0026, -0.0019,  ...,  0.0302,  0.0276, -0.0094],\n",
       "        [ 0.0062, -0.0029, -0.0237,  ...,  0.0011,  0.0062, -0.0046],\n",
       "        [ 0.0033,  0.0239, -0.0286,  ..., -0.0235, -0.0094, -0.0135],\n",
       "        ...,\n",
       "        [-0.0302, -0.0083,  0.0355,  ..., -0.0147,  0.0258, -0.0147],\n",
       "        [-0.0194,  0.0175, -0.0292,  ...,  0.0109, -0.0126,  0.0329],\n",
       "        [ 0.0252, -0.0354,  0.0275,  ..., -0.0344, -0.0097,  0.0229]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60961e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0311, -0.0306, -0.0240,  0.0032,  0.0047,  0.0091,  0.0204,  0.0016,\n",
       "        -0.0283, -0.0335], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0221",
   "metadata": {},
   "source": [
    "여기에는 총 7850개의 매개변수가 있지만 개념적으로는 지금까지 변경된 것이 없다.\n",
    "\n",
    "모델을 사용하여 일부 출력을 생성해 보겠다.\n",
    "\n",
    "데이터 세트에서 100개의 이미지 중 첫 번째 배치를 가져와서 모델에 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88fd8d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 5, 3, 8, 4, 5, 3, 9, 1, 2, 2, 8, 7, 2, 6, 8, 4, 1, 7, 9, 2, 8, 9,\n",
      "        7, 4, 5, 9, 0, 1, 6, 3, 7, 3, 8, 7, 3, 2, 6, 4, 0, 5, 4, 1, 9, 5, 0, 6,\n",
      "        5, 6, 8, 2, 6, 8, 5, 1, 8, 5, 0, 4, 0, 5, 7, 4, 9, 4, 1, 4, 9, 9, 4, 5,\n",
      "        1, 1, 7, 1, 3, 2, 0, 5, 8, 5, 5, 0, 9, 5, 9, 1, 5, 4, 1, 1, 1, 3, 8, 0,\n",
      "        9, 3, 3, 4, 2, 2, 4, 5, 1, 7, 8, 8, 2, 8, 4, 8, 8, 3, 8, 3, 0, 2, 6, 6,\n",
      "        9, 9, 7, 4, 9, 7, 8, 8])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d0fe7d306f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306be2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7360dfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.reshape(128, 784).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d66394",
   "metadata": {},
   "source": [
    "위의 코드는 입력 데이터의 모양이 맞지 않기 때문에 오류가 발생한다.\n",
    "\n",
    "우리의 이미지는 1x28x28 모양이지만 크기가 784인 벡터가 필요하다. 즉, 평면화해야한다.\n",
    "\n",
    "텐서의 `.reshape`메서드를 사용하면 기본 데이터의 복사본을 실제로 생성하지 않고도 각 이미지를 평면 벡터로 효율적으로 `볼` 수 있다.\n",
    "\n",
    "이 추가 기능을 모델에 포함하려면 PyTorch에서 `nn.Module` 클래스를 확장하여 사용자 정의 모델을 정의해야 한다.\n",
    "\n",
    "\n",
    "Python의 클래스는 객체 생성을 위한 `\"청사진\"`을 제공한다. Python에서 새 클래스를 정의하는 예를 살펴보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296ad41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    # Class constructor\n",
    "    def __init__(self, name, age):\n",
    "        # Object properties\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    \n",
    "    # Method\n",
    "    def say_hello(self):\n",
    "        print(\"Hello my name is \" + self.name + \"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459b834",
   "metadata": {},
   "source": [
    "다음은 `Person` 클래스의 객체를 생성하거나 _인스턴스화_하는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72d8a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = Person(\"Bob\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00181d9c",
   "metadata": {},
   "source": [
    "`bob` 객체는 `Person` 클래스의 인스턴스이다.\n",
    "\n",
    "객체의 속성(속성이라고도 함)에 액세스하거나 `.` 표기법을 사용하여 해당 메서드를 호출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0b4906c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bob', 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.name, bob.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c271a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Bob!\n"
     ]
    }
   ],
   "source": [
    "bob.say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc12b95",
   "metadata": {},
   "source": [
    "https://www.w3schools.com/python/python_classes.asp에서 Python 클래스에 대해 자세히 알아볼 수 있다.\n",
    "\n",
    "또한 클래스는 기존 클래스의 기능을 기반으로 하거나 _확장_할 수 있다. PyTorch에서 `nn.Module` 클래스를 확장하여 사용자 정의 모델을 정의해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b46073a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b794f2",
   "metadata": {},
   "source": [
    "`__init__` 생성자 메서드 내에서 `nn.Linear`를 사용하여 가중치와 편향을 인스턴스화합니다.\n",
    "\n",
    "그리고 입력 일괄 처리를 모델에 전달할 때 호출되는 `forward`메서드 내부에서 입력 텐서를 평평하게 만들고 이를 `self.linear`에 전달한다.\n",
    "\n",
    "`xb.reshape(-1, 28*28)`은 2차원의 `xb` 텐서의 *보기*를 원한다는 것을 PyTorch에 나타낸다.\n",
    "\n",
    "2차원의 길이는 28\\*28(즉, 784)이다.\n",
    "\n",
    "`.reshape`에 대한 한 인수를 `-1`(이 경우 첫 번째 차원)로 설정하면 PyTorch가 원래 텐서의 모양을 기반으로 자동으로 알아낼 수 있다.\n",
    "\n",
    "모델에는 더 이상 `.weight` 및 `.bias` 속성이 없지만(이제 `.linear` 속성 안에 있으므로) 가중치와 편향을 포함하는 목록을 반환하는 `.parameters` 메서드가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d07b46bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "448f3cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0229, -0.0177,  0.0315,  ...,  0.0339,  0.0163, -0.0160],\n",
       "         [-0.0059,  0.0097,  0.0205,  ..., -0.0029,  0.0007, -0.0280],\n",
       "         [ 0.0053,  0.0273,  0.0319,  ...,  0.0100, -0.0268,  0.0163],\n",
       "         ...,\n",
       "         [ 0.0261, -0.0101, -0.0135,  ..., -0.0090,  0.0053,  0.0092],\n",
       "         [-0.0304, -0.0119, -0.0262,  ...,  0.0343,  0.0258, -0.0093],\n",
       "         [ 0.0076, -0.0243,  0.0294,  ..., -0.0248, -0.0305,  0.0161]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0247, -0.0292, -0.0042,  0.0335,  0.0072, -0.0030,  0.0185,  0.0234,\n",
       "          0.0070,  0.0052], requires_grad=True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506c05d",
   "metadata": {},
   "source": [
    "우리는 이전과 같은 방식으로 새로운 커스텀 모델을 사용할 수 있다.\n",
    "\n",
    "작동하는지 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "258a1e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs :\n",
      " tensor([[-0.2213,  0.1604, -0.2281,  0.0992, -0.0449, -0.2389, -0.0427,  0.4363,\n",
      "          0.1643, -0.3218],\n",
      "        [-0.0171,  0.0877, -0.3320,  0.1971,  0.1721, -0.1101,  0.0245,  0.2728,\n",
      "          0.0228, -0.1300]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e6246",
   "metadata": {},
   "source": [
    "100개의 입력 이미지 각각에 대해 각 클래스에 대해 하나씩 10개의 출력을 얻는다.\n",
    "\n",
    "앞서 논의한 바와 같이, 우리는 이러한 출력이 확률을 나타내기를 원한다.\n",
    "\n",
    "각 출력 행의 요소는 0에서 1 사이에 있어야 하며 합이 1이 되어야한다.\n",
    "\n",
    "출력 행을 확률로 변환하기 위해 다음 공식을 갖는 softmax 함수를 사용한다.\n",
    "\n",
    "![소프트맥스](https://i.imgur.com/EAh9jLN.png)\n",
    "\n",
    "먼저 출력 행의 각 요소 `yi`를 `e^yi`로 교체하여 모든 요소를 양수로 만든다.\n",
    "\n",
    "![](https://www.montereyinstitute.org/courses/DevelopmentalMath/COURSE_TEXT2_RESOURCE/U18_L1_T1_text_final_6_files/image001.png)\n",
    "\n",
    "그런 다음 합이 1이 되도록 합으로 나눈다. 따라서 결과 벡터는 확률로 해석될 수 있다.\n",
    "\n",
    "softmax 함수를 구현하는 것은 쉽지만(시도해야 합니다!), 다차원 텐서(이 경우 출력 행 목록)와 잘 작동하기 때문에 PyTorch 내에서 제공되는 구현을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f4b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb23884",
   "metadata": {},
   "source": [
    "softmax 함수는 `torch.nn.functional` 패키지에 포함되어 있으며 함수를 적용해야 하는 차원을 지정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c56b9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2213,  0.1604, -0.2281,  0.0992, -0.0449, -0.2389, -0.0427,  0.4363,\n",
       "          0.1643, -0.3218],\n",
       "        [-0.0171,  0.0877, -0.3320,  0.1971,  0.1721, -0.1101,  0.0245,  0.2728,\n",
       "          0.0228, -0.1300]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3004a6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probabilities:\n",
      " tensor([[0.0799, 0.1171, 0.0794, 0.1101, 0.0953, 0.0785, 0.0955, 0.1543, 0.1175,\n",
      "         0.0723],\n",
      "        [0.0951, 0.1056, 0.0694, 0.1179, 0.1150, 0.0867, 0.0992, 0.1271, 0.0990,\n",
      "         0.0850]])\n",
      "Sum:  1.000000238418579\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax for each output row\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b13451",
   "metadata": {},
   "source": [
    "마지막으로 각 출력 행에서 확률이 가장 높은 요소의 인덱스를 선택하기만 하면 각 이미지에 대한 예측 레이블을 결정할 수 있다.\n",
    "\n",
    "각 행의 가장 큰 요소와 해당 인덱스를 반환하는 `torch.max`를 사용하여 이를 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e48d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 7, 3, 7, 7, 7, 5, 7, 7, 7, 8, 7, 7, 7, 7, 7, 4, 4, 8, 7, 7, 4, 7, 8,\n",
      "        0, 8, 7, 7, 7, 9, 7, 7, 4, 1, 9, 4, 9, 7, 3, 8, 7, 7, 7, 4, 8, 8, 1, 0,\n",
      "        9, 7, 7, 8, 7, 7, 4, 8, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7,\n",
      "        8, 7, 4, 7, 5, 0, 9, 4, 7, 0, 7, 7, 7, 7, 4, 7, 7, 1, 2, 8, 7, 7, 7, 5,\n",
      "        0, 3, 7, 8, 7, 7, 1, 7, 1, 7, 0, 4, 3, 9, 3, 4, 7, 7, 7, 2, 8, 7, 4, 7,\n",
      "        7, 9, 7, 7, 4, 7, 0, 7])\n",
      "tensor([0.1543, 0.1271, 0.1253, 0.1368, 0.1524, 0.1287, 0.1266, 0.1298, 0.1316,\n",
      "        0.1464, 0.1167, 0.1411, 0.1365, 0.1410, 0.1389, 0.1313, 0.1131, 0.1463,\n",
      "        0.1129, 0.1345, 0.1347, 0.1221, 0.1192, 0.1125, 0.1293, 0.1127, 0.1325,\n",
      "        0.1232, 0.1349, 0.1313, 0.1259, 0.1192, 0.1171, 0.1187, 0.1231, 0.1320,\n",
      "        0.1179, 0.1149, 0.1306, 0.1182, 0.1421, 0.1378, 0.1189, 0.1579, 0.1141,\n",
      "        0.1247, 0.1137, 0.1307, 0.1213, 0.1525, 0.1291, 0.1424, 0.1243, 0.1275,\n",
      "        0.1282, 0.1364, 0.1387, 0.1335, 0.1457, 0.1195, 0.1426, 0.1276, 0.1433,\n",
      "        0.1171, 0.1574, 0.1464, 0.1274, 0.1477, 0.1291, 0.1360, 0.1273, 0.1631,\n",
      "        0.1242, 0.1380, 0.1217, 0.1196, 0.1240, 0.1287, 0.1241, 0.1267, 0.1518,\n",
      "        0.1333, 0.1233, 0.1263, 0.1192, 0.1170, 0.1427, 0.1251, 0.1249, 0.1221,\n",
      "        0.1250, 0.1461, 0.1442, 0.1342, 0.1248, 0.1306, 0.1229, 0.1468, 0.1201,\n",
      "        0.1252, 0.1279, 0.1626, 0.1175, 0.1468, 0.1283, 0.1204, 0.1347, 0.1404,\n",
      "        0.1188, 0.1315, 0.1269, 0.1270, 0.1268, 0.1295, 0.1281, 0.1243, 0.1179,\n",
      "        0.1258, 0.1455, 0.1258, 0.1329, 0.1123, 0.1194, 0.1327, 0.1170, 0.1513,\n",
      "        0.1256, 0.1297], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd2307",
   "metadata": {},
   "source": [
    "위에 인쇄된 숫자는 훈련 이미지의 첫 번째 배치에 대한 예측된 레이블이다.\n",
    "\n",
    "실제 라벨과 비교해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bd3a894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 4, 3, 0, 8, 0, 0, 2, 9, 2, 3, 7, 7, 4, 0, 9, 9, 7, 1, 4, 2, 3, 4, 1,\n",
       "        3, 1, 8, 6, 5, 1, 7, 5, 1, 8, 1, 9, 1, 4, 7, 6, 2, 8, 6, 8, 6, 9, 8, 6,\n",
       "        1, 8, 2, 8, 2, 7, 4, 5, 5, 4, 3, 6, 8, 6, 3, 1, 0, 6, 6, 5, 8, 2, 7, 8,\n",
       "        5, 6, 3, 1, 6, 6, 1, 3, 2, 6, 2, 8, 2, 0, 9, 4, 7, 6, 5, 8, 4, 9, 0, 6,\n",
       "        8, 7, 0, 2, 4, 4, 1, 7, 6, 1, 4, 7, 5, 2, 2, 7, 5, 4, 2, 3, 6, 5, 9, 4,\n",
       "        0, 1, 4, 2, 1, 7, 6, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3bc58",
   "metadata": {},
   "source": [
    "대부분의 예측된 레이블은 실제 레이블과 다르다. 무작위로 초기화된 가중치와 편향으로 시작했기 때문이다.\n",
    "\n",
    "모델을 훈련시켜야한다. 즉, 더 나은 예측을 위해 경사하강법을 사용하여 가중치를 조정해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831539d",
   "metadata": {},
   "source": [
    "## 평가 메트릭 및 손실 함수\n",
    "\n",
    "선형 회귀와 마찬가지로 모델의 성능을 평가할 방법이 필요하다.\n",
    "\n",
    "이를 수행하는 자연스러운 방법은 올바르게 예측된 레이블의 백분율(예측의 **정확도**)을 찾는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5404f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2213,  0.1604, -0.2281,  0.0992, -0.0449, -0.2389, -0.0427,  0.4363,\n",
       "          0.1643, -0.3218],\n",
       "        [-0.0171,  0.0877, -0.3320,  0.1971,  0.1721, -0.1101,  0.0245,  0.2728,\n",
       "          0.0228, -0.1300]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96bf10ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23cc8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee6449",
   "metadata": {},
   "source": [
    "`==` 연산자는 모양이 같은 두 텐서의 요소별 비교를 수행하고 같지 않은 요소에 대해 `True`를 포함하고 동일한 요소에 대해 `False`를 포함하는 동일한 모양의 텐서를 반환한다. 결과를 `torch.sum`에 전달하면 올바르게 예측된 레이블 수를 반환한다. 마지막으로 정확도를 얻기 위해 총 이미지 수로 나눈다.\n",
    "\n",
    "\n",
    "결과가 동일한 상대 순서를 가지므로 출력에 softmax를 적용할 필요가 없다. 이는 `e^x`가 증가 함수이기 때문이다. 즉, `y1 > y2`이면 `e^y1 > e^y2`이다. softmax를 얻기 위해 값을 평균화한 후에도 마찬가지다.\n",
    "\n",
    "데이터의 첫 번째 배치에서 현재 모델의 정확도를 계산해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7fbc5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1016)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8238bffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0799, 0.1171, 0.0794,  ..., 0.1543, 0.1175, 0.0723],\n",
       "        [0.0951, 0.1056, 0.0694,  ..., 0.1271, 0.0990, 0.0850],\n",
       "        [0.0968, 0.1162, 0.0988,  ..., 0.1173, 0.1054, 0.0942],\n",
       "        ...,\n",
       "        [0.0973, 0.0981, 0.0985,  ..., 0.1513, 0.0930, 0.0922],\n",
       "        [0.1256, 0.1017, 0.0779,  ..., 0.0982, 0.1165, 0.0775],\n",
       "        [0.1119, 0.1058, 0.0907,  ..., 0.1297, 0.1204, 0.0736]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d2f0d4",
   "metadata": {},
   "source": [
    "정확도는 우리가 모델을 평가하는 훌륭한 방법이다. 그러나 다음과 같은 이유로 경사하강법을 사용하여 모델을 최적화하기 위한 손실 함수로 사용할 수 없다.\n",
    "\n",
    "1. 미분이 되지 않음 `torch.max`와 `==`는 모두 비연속적이고 미분할 수 없는 연산이므로 가중치와 편향으로 기울기를 계산하는 데 정확도를 사용할 수 없음\n",
    "\n",
    "2. 모델에 의해 예측된 실제 확률을 고려하지 않으므로 점진적 개선에 대한 충분한 피드백을 제공할 수 없음.\n",
    "\n",
    "\n",
    "이러한 이유로 종종 정확도는 분류 평가 지표로 활용되긴하지만, 손실함수로 사용되지 않는다.  분류 문제에 일반적으로 사용되는 손실 함수는 **교차 엔트로피**이며 아래에 공식이 있다.\n",
    "\n",
    "![크로스 엔트로피](https://i.imgur.com/VDRDl1D.png)\n",
    "\n",
    "복잡해 보이지만 실제로는 매우 간단하다.\n",
    "\n",
    "* 각 출력 행에 대해 올바른 레이블에 대한 예측 확률을 선택한다.\n",
    "\n",
    "예를 들어 이미지에 대한 예측 확률이 `[0.1, 0.3, 0.2, ...]`이고 올바른 레이블이 `1`이면 해당 요소 `0.3`을 선택하고 나머지는 무시한다.\n",
    "\n",
    "* 그런 다음 선택한 확률의 [logarithm](https://en.wikipedia.org/wiki/Logarithm)을 취한다.\n",
    "\n",
    "확률이 높으면, 즉 1에 가까우면 로그는 0에 가까운 매우 작은 음수 값이다. 그리고 확률이 낮으면(0에 가까움) 로그는 매우 큰 음수 값이다. 또한 결과에 -1을 곱한다. 이는 결과가 좋지 않은 예측에 대한 손실의 큰 양수 값이다.\n",
    "\n",
    "\n",
    "![](https://www.intmath.com/blog/wp-content/images/2019/05/log10.png)\n",
    "\n",
    "* 마지막으로 모든 출력 행에 대한 교차 엔트로피의 평균을 취하여 데이터 배치에 대한 전체 손실을 얻는다.\n",
    "\n",
    "정확도와 달리 교차 엔트로피는 연속적이고 미분 가능한 함수다. 또한 모델의 점진적 개선에 대한 유용한 피드백을 제공한다(정확한 레이블에 대한 약간 더 높은 확률은 더 낮은 손실로 이어짐). 이 두 가지 요소는 교차 엔트로피를 손실 함수에 더 나은 선택으로 만든다.\n",
    "\n",
    "\n",
    "예상할 수 있듯이 PyTorch는 `torch.nn.functional` 패키지의 일부로 효율적이고 텐서 친화적인 교차 엔트로피 구현을 제공한다. 또한 내부적으로 softmax를 수행하므로 확률로 변환하지 않고 모델의 출력을 직접 전달할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e01e89d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2213,  0.1604, -0.2281,  ...,  0.4363,  0.1643, -0.3218],\n",
       "        [-0.0171,  0.0877, -0.3320,  ...,  0.2728,  0.0228, -0.1300],\n",
       "        [-0.0406,  0.1420, -0.0203,  ...,  0.1516,  0.0442, -0.0680],\n",
       "        ...,\n",
       "        [-0.0916, -0.0834, -0.0794,  ...,  0.3498, -0.1367, -0.1454],\n",
       "        [ 0.2405,  0.0297, -0.2364,  ..., -0.0056,  0.1652, -0.2425],\n",
       "        [ 0.0627,  0.0069, -0.1473,  ...,  0.2104,  0.1357, -0.3558]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fc4532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c520413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3075, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b034a9d",
   "metadata": {},
   "source": [
    "우리는 교차 엔트로피가 모든 훈련 샘플에 대해 평균화된 올바른 레이블의 예측 확률의 음의 로그라는 것을 알고 있다. 따라서 결과 숫자를 해석하는 한 가지 방법은 다음과 같습니다. `2.23`은 `e^-2.23`을 보면 평균적으로 올바른 레이블의 예측 확률이 `0.1` 정도다.\n",
    "\n",
    "*손실이 낮을수록 좋은 모델*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab67ec0",
   "metadata": {},
   "source": [
    "## 모델 훈련\n",
    "\n",
    "데이터 로더, 모델, 손실 함수 및 옵티마이저를 정의했으므로 이제 모델을 훈련할 준비가 되었다.\n",
    "\n",
    "훈련 과정은 각 시대의 모델을 평가하기 위해 \"검증 단계\"가 추가된 선형 회귀와 동일하다. 아래에 의사코드를 나타냈음:\n",
    "\n",
    "```\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    for batch in train_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Compute gradients\n",
    "        # Update weights\n",
    "        # Reset gradients\n",
    "    \n",
    "    # Validation phase\n",
    "    for batch in val_loader:\n",
    "        # Generate predictions\n",
    "        # Calculate loss\n",
    "        # Calculate metrics (accuracy etc.)\n",
    "    # Calculate average validation loss & metrics\n",
    "    \n",
    "    # Log epoch, loss & metrics for inspection\n",
    "```\n",
    "\n",
    "훈련 루프의 일부는 우리가 해결하고 있는 특정 문제(예: 손실 함수, 메트릭 등)에 특정한 반면 다른 부분은 일반적이며 모든 딥 러닝 문제에 적용할 수 있다. 모델을 훈련하는 데 사용할 `fit`이라는 함수에 문제와 무관한 부분을 포함할 것이다. \n",
    "\n",
    "문제별 부분은 `nn.Module` 클래스에 새 메서드를 추가하여 구현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b630c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf344c",
   "metadata": {},
   "source": [
    "`fit`함수는 각 에포크의 유효성 검사 손실과 메트릭을 기록한다. 디버깅 및 시각화에 유용한 교육 기록을 반환한다.\n",
    "\n",
    "배치 크기, 학습률 등과 같은 구성(초매개변수라고 함)은 기계 학습 모델을 학습하는 동안 미리 선택해야 한다.\n",
    "적절한 초매개변수를 선택하는 것은 합리적인 시간 내에 합리적으로 정확한 모델을 훈련하는 데 중요하다. 머신 러닝의 연구 및 실험이 활발한 영역이다. 자유롭게 다양한 학습률을 시도하고 이것이 교육 과정에 어떤 영향을 미치는지 확인해보자.\n",
    "\n",
    "`fit`의 유효성 검사 단계에서 사용되는 `evaluate`함수를 정의해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52dd9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b0fe932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = [x*2 for x in l1]\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae64e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e1e17",
   "metadata": {},
   "source": [
    "마지막으로 `fit` 및 `evaluate`에서 사용하는 추가 메소드 `training_step`, `validation_step`, `validation_epoch_end`, `epoch_end`를 포함하도록 `MnistModel` 클래스를 재정의해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "325add75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5536b",
   "metadata": {},
   "source": [
    "모델을 훈련시키기 전에 무작위로 초기화된 가중치 및 편향의 초기 세트를 사용하여 검증 세트에서 모델이 어떻게 수행되는지 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae0b01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3294854164123535, 'val_acc': 0.06744462251663208}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee69443",
   "metadata": {},
   "source": [
    "초기 정확도는 약 10%로 무작위로 초기화된 모델에서 기대할 수 있다(무작위로 추측하여 올바른 레이블을 얻을 확률이 10분의 1이기 때문에).\n",
    "\n",
    "이제 모델을 훈련할 준비가 되었다. 5개의 Epoch 동안 훈련하고 결과를 살펴보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1cf31c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9630, val_acc: 0.6008\n",
      "Epoch [1], val_loss: 1.6926, val_acc: 0.7300\n",
      "Epoch [2], val_loss: 1.4901, val_acc: 0.7661\n",
      "Epoch [3], val_loss: 1.3368, val_acc: 0.7868\n",
      "Epoch [4], val_loss: 1.2187, val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26779d77",
   "metadata": {},
   "source": [
    "좋은 결과입니다! 단 5개의 Epoch 훈련으로 우리 모델은 검증 세트에서 80% 이상의 정확도에 도달했다.\n",
    "\n",
    "몇 에포크를 더 훈련하여 개선할 수 있는지 보자.\n",
    "\n",
    "아래의 각 셀에서 학습률과 Epoch 수를 변경해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06262b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.9392, val_acc: 0.8278\n",
      "Epoch [1], val_loss: 0.8962, val_acc: 0.8314\n",
      "Epoch [2], val_loss: 0.8594, val_acc: 0.8348\n",
      "Epoch [3], val_loss: 0.8276, val_acc: 0.8384\n",
      "Epoch [4], val_loss: 0.7997, val_acc: 0.8403\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c57b2330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7751, val_acc: 0.8425\n",
      "Epoch [1], val_loss: 0.7532, val_acc: 0.8452\n",
      "Epoch [2], val_loss: 0.7336, val_acc: 0.8469\n",
      "Epoch [3], val_loss: 0.7159, val_acc: 0.8486\n",
      "Epoch [4], val_loss: 0.6999, val_acc: 0.8504\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95e4fdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.6853, val_acc: 0.8527\n",
      "Epoch [1], val_loss: 0.6719, val_acc: 0.8542\n",
      "Epoch [2], val_loss: 0.6596, val_acc: 0.8554\n",
      "Epoch [3], val_loss: 0.6483, val_acc: 0.8558\n",
      "Epoch [4], val_loss: 0.6377, val_acc: 0.8568\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1eecd371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221ba45",
   "metadata": {},
   "source": [
    "위의 그림을 보면 모델이 아주 오랜 시간 동안 훈련한 후에도 정확도 임계값 90%를 넘지 않을 것이라는 점을 아주 분명하게 알 수 있다. 이에 대한 한 가지 가능한 이유는 학습률이 너무 높을 수 있기 때문이다. 모델의 매개변수는 가장 낮은 손실을 위한 최적의 매개변수 세트를 중심으로 `\"바운스\"`할 수 있다. 학습률을 줄이고 몇 에포크를 더 훈련하여 도움이 되는지 확인할 수 있다.\n",
    "\n",
    "**모델이 충분히 강력하지 않은** 이유가 더 많다. 초기 가설을 기억한다면 출력(이 경우 클래스 확률)은 입력(픽셀 강도)과 가중치 행렬 곱을 수행하고 편향을 더한 결과인 **선형 함수**이다. 이미지의 픽셀 강도와 이미지가 나타내는 숫자 사이에 선형 관계가 실제로 존재하지 않을 수 있으므로 이것은 상당히 약한 가정이다.\n",
    "\n",
    "MNIST(85% 정확도 제공)와 같은 간단한 데이터 세트에서는 합리적으로 잘 작동하지만 일상적인 물체, 동물 등을 인식하는 것과 같은 복잡한 작업을 위해 이미지 픽셀과 레이블 간의 비선형 관계를 캡처할 수 있는 보다 정교한 모델이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5ca52",
   "metadata": {},
   "source": [
    "## 개별 이미지로 테스트\n",
    "\n",
    "지금까지 모델의 전체 정확도를 추적했지만 일부 샘플 이미지에서 모델의 결과를 확인하는 것도 좋은 생각일 것이다.\n",
    "\n",
    "미리 정의된 10000개 이미지의 테스트 데이터 세트에서 일부 이미지로 모델을 테스트해 보자.\n",
    "\n",
    "`ToTensor`변환을 사용하여 테스트 데이터 세트를 다시 만드는 것으로 시작하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad4ccdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test dataset\n",
    "test_dataset = MNIST(root='data/', \n",
    "                     train=False,\n",
    "                     transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c06803d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abad10",
   "metadata": {},
   "source": [
    "단일 이미지 텐서에 대해 예측된 레이블을 반환하는 도우미 함수 `predict_image`를 정의해 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94c525a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f6e56",
   "metadata": {},
   "source": [
    "`img.unsqueeze`는 1x28x28 텐서의 시작 부분에 다른 차원을 추가하여 1x1x28x28 텐서를 만들고 모델이 단일 이미지를 포함하는 배치로 보자.\n",
    "\n",
    "몇 개의 이미지로 시도해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d63aa30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 , Predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "720325eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 , Predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "43971854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9 , Predicted: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[193]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5efc6d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 , Predicted: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[1839]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95651572",
   "metadata": {},
   "source": [
    "모델 성능이 저조한 위치를 식별하면 더 많은 교육 데이터를 수집하고 모델의 복잡성을 늘리거나 줄이며 하이퍼파라미터를 변경하여 모델을 개선하는 데 도움이 될 수 있다.\n",
    "\n",
    "마지막 단계로 테스트 세트에서 모델의 전체 손실과 정확도도 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f417dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.606325089931488, 'val_acc': 0.865039050579071}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8926a",
   "metadata": {},
   "source": [
    "이것이 검증 세트의 정확도/손실과 유사할 것으로 예상된다.\n",
    "\n",
    "그렇지 않은 경우 테스트 세트(종종 실제 데이터에서 가져옴)와 유사한 데이터 및 분포를 가진 더 나은 검증 세트가 필요할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648925e8",
   "metadata": {},
   "source": [
    "## 모델 저장 및 불러오기\n",
    "\n",
    "우리는 오랜 시간 동안 모델을 훈련했고 상당한 정확도를 달성했기 때문에 나중에 모델을 재사용하고 처음부터 다시 훈련하지 않도록 가중치와 편향 행렬을 디스크에 저장하는 것이 좋다. \n",
    "\n",
    "모델을 저장하는 방법은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7135eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ff8a8",
   "metadata": {},
   "source": [
    "`.state_dict` 메소드는 모델의 올바른 속성에 매핑된 모든 가중치와 편향 행렬을 포함하는 `OrderedDict`를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5aef237d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 1.5897e-03,  1.4094e-02,  6.9776e-03,  ...,  1.0001e-02,\n",
       "                        4.2146e-04,  5.3545e-03],\n",
       "                      [-1.2061e-02, -1.0422e-02, -2.0551e-02,  ..., -2.6979e-03,\n",
       "                       -3.5059e-02, -2.0220e-02],\n",
       "                      [-4.5509e-04,  4.6315e-03,  2.2319e-02,  ...,  1.2776e-02,\n",
       "                        3.2915e-02, -7.8641e-03],\n",
       "                      ...,\n",
       "                      [ 5.4894e-03, -3.2955e-02, -9.4656e-05,  ..., -2.0017e-02,\n",
       "                        9.5782e-03, -1.5799e-02],\n",
       "                      [-2.1682e-02,  2.8357e-02,  3.2435e-02,  ..., -3.4468e-02,\n",
       "                       -1.9810e-02,  3.5382e-02],\n",
       "                      [ 1.8925e-02,  2.3795e-02,  2.5661e-02,  ..., -9.8993e-03,\n",
       "                       -2.1034e-03,  9.2217e-03]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0790,  0.0707, -0.0161, -0.0241,  0.0467,  0.0795, -0.0009,  0.0380,\n",
       "                      -0.0811,  0.0117]))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db10a1e4",
   "metadata": {},
   "source": [
    "모델 가중치를 로드하기 위해 `MnistModel` 클래스의 새 객체를 인스턴스화하고 `.load_state_dict` 메소드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eba25d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a017ebef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 0.0036,  0.0042, -0.0357,  ..., -0.0117,  0.0206, -0.0184],\n",
       "                      [ 0.0281, -0.0338, -0.0348,  ...,  0.0302, -0.0154, -0.0303],\n",
       "                      [ 0.0166,  0.0173,  0.0341,  ...,  0.0017,  0.0085, -0.0300],\n",
       "                      ...,\n",
       "                      [-0.0128, -0.0042, -0.0291,  ..., -0.0113,  0.0143,  0.0338],\n",
       "                      [-0.0294,  0.0166,  0.0178,  ...,  0.0318,  0.0100, -0.0081],\n",
       "                      [ 0.0144, -0.0150, -0.0219,  ...,  0.0018, -0.0261,  0.0130]])),\n",
       "             ('linear.bias',\n",
       "              tensor([ 0.0312, -0.0247, -0.0163, -0.0255, -0.0239,  0.0192,  0.0031,  0.0338,\n",
       "                       0.0332, -0.0274]))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48c7cc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3144640922546387, 'val_acc': 0.12509766221046448}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2ae0390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[ 1.5897e-03,  1.4094e-02,  6.9776e-03,  ...,  1.0001e-02,\n",
       "                        4.2146e-04,  5.3545e-03],\n",
       "                      [-1.2061e-02, -1.0422e-02, -2.0551e-02,  ..., -2.6979e-03,\n",
       "                       -3.5059e-02, -2.0220e-02],\n",
       "                      [-4.5509e-04,  4.6315e-03,  2.2319e-02,  ...,  1.2776e-02,\n",
       "                        3.2915e-02, -7.8641e-03],\n",
       "                      ...,\n",
       "                      [ 5.4894e-03, -3.2955e-02, -9.4656e-05,  ..., -2.0017e-02,\n",
       "                        9.5782e-03, -1.5799e-02],\n",
       "                      [-2.1682e-02,  2.8357e-02,  3.2435e-02,  ..., -3.4468e-02,\n",
       "                       -1.9810e-02,  3.5382e-02],\n",
       "                      [ 1.8925e-02,  2.3795e-02,  2.5661e-02,  ..., -9.8993e-03,\n",
       "                       -2.1034e-03,  9.2217e-03]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0790,  0.0707, -0.0161, -0.0241,  0.0467,  0.0795, -0.0009,  0.0380,\n",
       "                      -0.0811,  0.0117]))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25979b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.606325089931488, 'val_acc': 0.865039050579071}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model2, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da9fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
